{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal,ogr,osr\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../aviris_data/subzone_d/d_ang20220709t214748rfl/data/hs_raw_image/ang20220709t214748_rfl_v2aa2_img\"\n",
    "hdr_file = f\"{file}.hdr\"\n",
    "# Check if files exist\n",
    "if os.path.exists(file):\n",
    "    print(f\"File exists: {file}\")\n",
    "else:\n",
    "    print(f\"File not found: {file}\")\n",
    "\n",
    "if os.path.exists(hdr_file):\n",
    "    print(f\"HDR File exists: {hdr_file}\")\n",
    "else:\n",
    "    print(f\"HDR File not found: {hdr_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Open the hyperspectral image\n",
    "img_file = file # Replace with your file path\n",
    "img_open = gdal.Open(img_file)\n",
    "\n",
    "# Step 2: Read the reflectance data (425 bands, 660 cols, 3647 rows)\n",
    "bands = img_open.RasterCount\n",
    "cols = img_open.RasterXSize\n",
    "rows = img_open.RasterYSize\n",
    "\n",
    "# Reading the reflectance data for each band (3D matrix of shape [bands, cols, rows])\n",
    "#reflectance_data = np.zeros((bands, rows, cols), dtype=np.float32)  # Fixed shape to [bands, rows, cols]\n",
    "\n",
    "reflectance_data = img_open.ReadAsArray()\n",
    "reflectance_data = np.where(reflectance_data == -9999, np.nan, reflectance_data)\n",
    "#for band in range(bands):\n",
    "#    reflectance_data[band] = img_open.GetRasterBand(band + 1).ReadAsArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `hyperspectral_data` is a 3D array (bands, height, width)\n",
    "# Select the band you want to plot (e.g., band 1)\n",
    "band_index = 57  # For the first band (adjust as needed)\n",
    "band_data = reflectance_data[band_index - 1]  # Convert to 0-based index\n",
    "\n",
    "# Plot the selected band\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(band_data, cmap='twilight_shifted')  # Use colormap 'gray' or any other as per preference\n",
    "plt.colorbar(label='Reflectance')  # Add a color bar to indicate values\n",
    "plt.title(f'Band {band_index}')\n",
    "plt.xlabel('X (pixels)')\n",
    "plt.ylabel('Y (pixels)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflectance_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define bands to cut out\n",
    "bblist = np.ones((425,))\n",
    "bblist[0:14] = 0\n",
    "bblist[189:225] = 0\n",
    "bblist[281:336] = 0\n",
    "bblist[405:] = 0\n",
    "\n",
    "# Define the pixel coordinates (row, column) you are interested in\n",
    "pixel_x = 250  # X-coordinate (column)\n",
    "pixel_y = 3500  # Y-coordinate (row)\n",
    "\n",
    "# Extract reflectance data for the specified pixel across all bands\n",
    "pixel_reflectance = reflectance_data[:, pixel_y, pixel_x]\n",
    "pixel_reflectance[bblist == 0] = np.nan\n",
    "\n",
    "# Plot the reflectance data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(pixel_reflectance, marker='none')\n",
    "plt.title(f'Reflectance Spectrum at Pixel ({pixel_x}, {pixel_y})')\n",
    "plt.xlabel('Band Number')\n",
    "plt.ylabel('Reflectance')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to (num_pixels, num_bands)\n",
    "pixels = reflectance_data.reshape((bands, -1)).T  # Shape: [num_pixels, num_bands]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Check for NaN values before attempting to remove\n",
    "print(f\"Shape of the original data: {pixels.shape}\")\n",
    "print(f\"NaNs in the original data: {np.sum(np.isnan(pixels))}\")\n",
    "\n",
    "# Step 5: Remove rows (pixels) with any NaN values\n",
    "pixels_clean = pixels[~np.isnan(pixels).any(axis=1)]  # Remove rows with any NaN values\n",
    "\n",
    "# Step 6: Ensure no NaN values are left\n",
    "print(f\"Shape after removal: {pixels_clean.shape}\")\n",
    "print(f\"NaNs after removal: {np.sum(np.isnan(pixels_clean))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "pixels_normalized = (pixels_clean - np.min(pixels_clean, axis=0)) / (np.max(pixels_clean, axis=0) - np.min(pixels_clean, axis=0) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "train_data, test_data = train_test_split(pixels_normalized, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "train_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "test_tensor = torch.tensor(test_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Any NaN in train_tensor:\", torch.isnan(train_tensor).any())\n",
    "print(\"Any Inf in train_tensor:\", torch.isinf(train_tensor).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim),\n",
    "            nn.Sigmoid(),  # To match normalized input\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n",
    "\n",
    "# Initialize model\n",
    "input_dim = bands\n",
    "latent_dim = 32  # Adjust this based on desired compression level\n",
    "model = Autoencoder(input_dim=input_dim, latent_dim=latent_dim)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the model\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed = model(batch)\n",
    "        loss = criterion(reconstructed, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * batch.size(0)\n",
    "    train_loss /= len(train_tensor)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {train_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed_test = model(test_tensor)\n",
    "    test_loss = criterion(reconstructed_test, test_tensor).item()\n",
    "    print(f\"Test Loss: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract latent features\n",
    "latent_features = model.encoder(torch.tensor(pixels_normalized, dtype=torch.float32)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape latent features for visualization or clustering\n",
    "latent_image = latent_features.reshape((rows, cols, 29))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize one of the latent dimensions\n",
    "plt.imshow(latent_image[:, :, 0], cmap=\"viridis\")  # Visualize the first latent dimension\n",
    "plt.colorbar()\n",
    "plt.title(\"Latent Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
